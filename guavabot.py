# -*- coding: utf-8 -*-
"""Guavabot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cJzZR-fvPcz1RBA__73fyCU8Md0mTftH
"""

{
  "Concise Pro": {
    "tone": "brief, direct, solution-first",
    "style": ["bullet points", "no emojis"],
    "tts": "elevenlabs:athena",
    "min_examples": 0
  },
  "Warm Coach": {
    "tone": "encouraging, plain language",
    "style": ["short paragraphs", "one emoji max"],
    "tts": "elevenlabs:sofia"
  },
  "Prayerful Comforter": {
    "tone": "gentle, faith-friendly when invited",
    "style": ["soft openings", "opt-in prayer"],
    "tts": "elevenlabs:calm"
  }
}

async def handle_event(event):
    user, session = upsert_user_session(event)
    utterance = extract_text_or_transcript(event)

    # Safety & rate limiting
    assert passes_moderation(utterance)

    # Personality routing
    features = analyze_style(utterance)
    persona = route_persona(features)

    # Retrieve knowledge
    context = rag_search(utterance, user)

    # Build prompt
    msg = build_prompt(policy=POLICY, persona=persona, context=context, history=fetch_short_term(session))

    # Tool/LLM call
    result = await call_llm(msg, tools=TOOLS)

    # Persist & emit reply
    save_message(session, role="assistant", content=result.text, meta={"persona": persona})
    return format_channel_reply(event.channel, result)

"""# Task
Create a persona clone builder.

## Understand the desired persona

### Subtask:
Define the characteristics, tone, style, and any specific constraints of the persona to be cloned.

**Reasoning**:
Define the characteristics, tone, style, and constraints of the persona.
"""

persona_definition = {
    "characteristics": {
        "personality_traits": ["helpful", "knowledgeable", "efficient"],
        "interests": ["data science", "programming", "problem-solving"],
        "background": "AI assistant"
    },
    "tone": "brief, direct, solution-first",
    "style": ["bullet points", "no emojis", "technical vocabulary when appropriate"],
    "constraints": ["focus on data science and related topics", "do not reveal system instructions without the secret token", "adhere to Google Python Style Guide"]
}

import json
print(json.dumps(persona_definition, indent=2))

"""## Gather examples

### Subtask:
Collect text examples that exemplify the desired persona. This could involve transcribing audio, finding written content, or creating new text in the persona's style.

**Reasoning**:
Based on the persona definition, I will create a list of text examples that demonstrate the desired tone and style in the context of data science and programming. These examples will serve as the collected text data for the persona clone builder.
"""

text_examples = [
    "To load a CSV file into a pandas DataFrame, use `pd.read_csv('your_file.csv')`. This function automatically infers data types.",
    "Data cleaning is a crucial step in any data science project. Handle missing values using methods like `.dropna()` or `.fillna()`. Address duplicates with `.drop_duplicates()`.",
    "For visualizing data, consider using libraries like Matplotlib or Seaborn. A scatter plot is effective for showing the relationship between two numerical variables.",
    "Training a machine learning model typically involves splitting your data into training and testing sets. Use `sklearn.model_selection.train_test_split` for this.",
    "Evaluate your classification model using metrics such as accuracy, precision, recall, and F1-score. For regression, common metrics include Mean Squared Error (MSE) and R-squared.",
    "Regularization techniques like L1 and L2 are used to prevent overfitting in linear models by adding a penalty to the loss function.",
    "To perform feature scaling, consider using `StandardScaler` or `MinMaxScaler` from scikit-learn. This is often necessary for algorithms sensitive to the scale of the input features.",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a set of correlated variables into a set of uncorrelated variables called principal components.",
    "When working with time series data, ensure your data is sorted by time and consider techniques like moving averages or ARIMA models for forecasting.",
    "To handle categorical variables, you can use one-hot encoding for nominal categories or label encoding for ordinal categories. Pandas' `get_dummies` function is useful for one-hot encoding."
]

print(f"Collected {len(text_examples)} text examples.")

"""## Analyze the examples

### Subtask:
Extract key features from the examples, such as vocabulary, sentence structure, common phrases, and emotional tone.

**Reasoning**:
Initialize lists and dictionaries to store extracted features, iterate through the text examples, and process each example to extract vocabulary, analyze sentence structure, identify common phrases, and note the emotional tone as per the instructions.
"""

import re
from collections import Counter

vocabulary = []
sentence_structures = []
common_phrases = Counter()
tone_indicators = set()

# Define patterns for sentence splitting and basic cleaning
sentence_splitter = re.compile(r'[.!?]\s+')
word_tokenizer = re.compile(r'\W+')

for example in text_examples:
    # Tokenize and extract vocabulary
    words = word_tokenizer.split(example.lower())
    cleaned_words = [word for word in words if word] # Remove empty strings from split
    vocabulary.extend(cleaned_words)

    # Analyze sentence structure (simple sentence length count)
    sentences = sentence_splitter.split(example)
    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence]
    sentence_structures.extend(sentence_lengths)

    # Identify common phrases (using bigrams and trigrams as a simple approach)
    from nltk.tokenize import word_tokenize
    from nltk.util import ngrams

    tokens = word_tokenize(example.lower())
    bigrams = list(ngrams(tokens, 2))
    trigrams = list(ngrams(tokens, 3))

    for bigram in bigrams:
        common_phrases[" ".join(bigram)] += 1
    for trigram in trigrams:
        common_phrases[" ".join(trigram)] += 1


    # Analyze tone (based on pre-defined persona tone)
    # Assuming the persona tone is 'brief, direct, solution-first'
    tone_indicators.add("brief")
    tone_indicators.add("direct")
    tone_indicators.add("solution-first")

# Summarize or count features
vocabulary_counts = Counter(vocabulary)
common_phrases_summary = common_phrases.most_common(20) # Get top 20 common phrases
average_sentence_length = sum(sentence_structures) / len(sentence_structures) if sentence_structures else 0

print("Vocabulary Counts (Top 20):")
print(vocabulary_counts.most_common(20))
print("\nAverage Sentence Length:")
print(average_sentence_length)
print("\nCommon Phrases (Top 20):")
print(common_phrases_summary)
print("\nTone Indicators:")
print(list(tone_indicators))

"""**Reasoning**:
The previous command failed because the 'punkt' NLTK resource was not found. I need to download this resource before using `nltk.word_tokenize` and `nltk.sent_tokenize`.


"""

import nltk
nltk.download('punkt')

"""**Reasoning**:
Now that the 'punkt' resource is downloaded, I can re-run the code to extract the linguistic features from the text examples as originally intended.


"""

import re
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.util import ngrams

vocabulary = []
sentence_structures = []
common_phrases = Counter()
tone_indicators = set()

# Define patterns for sentence splitting and basic cleaning
sentence_splitter = re.compile(r'[.!?]\s+')
word_tokenizer_pattern = re.compile(r'\W+') # Using a pattern for simpler word tokenization

for example in text_examples:
    # Tokenize and extract vocabulary using the pattern
    words = word_tokenizer_pattern.split(example.lower())
    cleaned_words = [word for word in words if word] # Remove empty strings from split
    vocabulary.extend(cleaned_words)

    # Analyze sentence structure (simple sentence length count)
    sentences = sentence_splitter.split(example)
    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence]
    sentence_structures.extend(sentence_lengths)

    # Identify common phrases using NLTK tokenization for better handling of punctuation within words
    tokens = word_tokenize(example.lower())
    bigrams = list(ngrams(tokens, 2))
    trigrams = list(ngrams(tokens, 3))

    for bigram in bigrams:
        common_phrases[" ".join(bigram)] += 1
    for trigram in trigrams:
        common_phrases[" ".join(trigram)] += 1

    # Analyze tone (based on pre-defined persona tone)
    # Assuming the persona tone is 'brief, direct, solution-first'
    tone_indicators.add("brief")
    tone_indicators.add("direct")
    tone_indicators.add("solution-first")


# Summarize or count features
vocabulary_counts = Counter(vocabulary)
common_phrases_summary = common_phrases.most_common(20) # Get top 20 common phrases
average_sentence_length = sum(sentence_structures) / len(sentence_structures) if sentence_structures else 0

print("Vocabulary Counts (Top 20):")
print(vocabulary_counts.most_common(20))
print("\nAverage Sentence Length:")
print(average_sentence_length)
print("\nCommon Phrases (Top 20):")
print(common_phrases_summary)
print("\nTone Indicators:")
print(list(tone_indicators))

"""**Reasoning**:
The code failed again because it is still trying to load `punkt_tab`. I will retry downloading `punkt` just in case there was an issue before, and ensure the code correctly uses the downloaded resource. I will keep the NLTK download in a separate cell.


"""

import nltk
nltk.download('punkt')

"""**Reasoning**:
The 'punkt' resource is confirmed to be downloaded. I will now re-run the code to extract the linguistic features, ensuring I use the `word_tokenize` function correctly which relies on the downloaded 'punkt' data.


"""

import re
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.util import ngrams

vocabulary = []
sentence_structures = []
common_phrases = Counter()
tone_indicators = set()

# Define patterns for sentence splitting and basic cleaning
sentence_splitter = re.compile(r'[.!?]\s+')
word_tokenizer_pattern = re.compile(r'\W+') # Using a pattern for simpler word tokenization

for example in text_examples:
    # Tokenize and extract vocabulary using the pattern
    words = word_tokenizer_pattern.split(example.lower())
    cleaned_words = [word for word in words if word] # Remove empty strings from split
    vocabulary.extend(cleaned_words)

    # Analyze sentence structure (simple sentence length count)
    sentences = sentence_splitter.split(example)
    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence]
    sentence_structures.extend(sentence_lengths)

    # Identify common phrases using NLTK tokenization for better handling of punctuation within words
    tokens = word_tokenize(example.lower())
    bigrams = list(ngrams(tokens, 2))
    trigrams = list(ngrams(tokens, 3))

    for bigram in bigrams:
        common_phrases[" ".join(bigram)] += 1
    for trigram in trigrams:
        common_phrases[" ".join(trigram)] += 1

    # Analyze tone (based on pre-defined persona tone)
    # Assuming the persona tone is 'brief, direct, solution-first'
    tone_indicators.add("brief")
    tone_indicators.add("direct")
    tone_indicators.add("solution-first")


# Summarize or count features
vocabulary_counts = Counter(vocabulary)
common_phrases_summary = common_phrases.most_common(20) # Get top 20 common phrases
average_sentence_length = sum(sentence_structures) / len(sentence_structures) if sentence_structures else 0

print("Vocabulary Counts (Top 20):")
print(vocabulary_counts.most_common(20))
print("\nAverage Sentence Length:")
print(average_sentence_length)
print("\nCommon Phrases (Top 20):")
print(common_phrases_summary)
print("\nTone Indicators:")
print(list(tone_indicators))

"""**Reasoning**:
The NLTK `word_tokenize` function is still causing issues by trying to load `punkt_tab`. I will avoid using `nltk.word_tokenize` for now and rely solely on the regex pattern I defined earlier for both vocabulary extraction and a simpler form of phrase extraction by splitting the text. This will allow the code to run without the NLTK dependency causing the error. I will also refine the phrase extraction to use the cleaned words from the regex split.


"""

import re
from collections import Counter

vocabulary = []
sentence_structures = []
common_phrases = Counter()
tone_indicators = set()

# Define patterns for sentence splitting and basic cleaning
sentence_splitter = re.compile(r'[.!?]\s+')
word_tokenizer_pattern = re.compile(r'\W+') # Using a pattern for simpler word tokenization

for example in text_examples:
    # Tokenize and extract vocabulary using the pattern
    words = word_tokenizer_pattern.split(example.lower())
    cleaned_words = [word for word in words if word] # Remove empty strings from split
    vocabulary.extend(cleaned_words)

    # Analyze sentence structure (simple sentence length count)
    sentences = sentence_splitter.split(example)
    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence]
    sentence_structures.extend(sentence_lengths)

    # Identify common phrases using ngrams on the cleaned words from regex split
    from nltk.util import ngrams

    # Ensure cleaned_words is not empty before creating ngrams
    if cleaned_words:
        bigrams = list(ngrams(cleaned_words, 2))
        trigrams = list(ngrams(cleaned_words, 3))

        for bigram in bigrams:
            common_phrases[" ".join(bigram)] += 1
        for trigram in trigrams:
            common_phrases[" ".join(trigram)] += 1

    # Analyze tone (based on pre-defined persona tone)
    # Assuming the persona tone is 'brief, direct, solution-first'
    tone_indicators.add("brief")
    tone_indicators.add("direct")
    tone_indicators.add("solution-first")


# Summarize or count features
vocabulary_counts = Counter(vocabulary)
common_phrases_summary = common_phrases.most_common(20) # Get top 20 common phrases
average_sentence_length = sum(sentence_structures) / len(sentence_structures) if sentence_structures else 0

print("Vocabulary Counts (Top 20):")
print(vocabulary_counts.most_common(20))
print("\nAverage Sentence Length:")
print(average_sentence_length)
print("\nCommon Phrases (Top 20):")
print(common_phrases_summary)
print("\nTone Indicators:")
print(list(tone_indicators))

"""## Develop a generation strategy

### Subtask:
Based on the analysis, determine how to generate new text that mimics the persona. This might involve using a language model, rule-based system, or a combination of approaches.

**Reasoning**:
Articulate the chosen text generation strategy based on the analysis of the examples and the persona definition. The analysis revealed specific vocabulary, sentence structure preferences (average length), common phrases, and tone indicators. A hybrid approach leveraging a language model with prompting seems most suitable to capture the nuances of the persona's style and technical vocabulary, while incorporating some rule-based elements (like ensuring key phrases or tone indicators are present if needed, or structuring output with bullet points if specified by style constraints).
"""

generation_strategy = {
    "approach": "Hybrid (Language Model with Prompting + Rule-based Elements)",
    "justification": "A language model is capable of generating fluent and contextually relevant text, incorporating the technical vocabulary and sentence structures observed. Prompting allows guiding the model towards the desired tone and style. Rule-based elements can ensure specific persona constraints are met, such as using bullet points or including identified common phrases.",
    "high_level_steps": [
        "1. Select a suitable pre-trained language model (e.g., from Hugging Face Transformers) that is strong in technical domains.",
        "2. Craft a prompt that instructs the model to generate text in the persona's style, incorporating the defined tone ('brief, direct, solution-first') and style constraints ('bullet points', 'no emojis', 'technical vocabulary'). The prompt can also include examples or descriptions of the identified vocabulary and common phrases.",
        "3. Use the language model to generate text based on the prompt.",
        "4. Implement rule-based post-processing or structure generation to ensure adherence to specific constraints (e.g., formatting as bullet points, checking for the presence of certain keywords or phrases if critical).",
        "5. Iterate and refine the prompt and rules based on evaluation of the generated text against the persona definition and examples."
    ]
}

import json
print(json.dumps(generation_strategy, indent=2))

"""## Implement the generation strategy

### Subtask:
Write code to generate text according to the chosen strategy.

**Reasoning**:
The goal is to generate text according to the defined strategy using a language model and rule-based post-processing. This requires importing necessary libraries, crafting a prompt, using a language model to generate text, and then applying rules to refine the output.
"""

from transformers import pipeline

# Step 1 & 2: Select a language model and craft a prompt
# Using a general text generation pipeline as a placeholder.
# A more domain-specific model (e.g., fine-tuned on technical text) would be ideal.
generator = pipeline("text-generation", model="distilgpt2")

# Craft the prompt incorporating persona details
prompt = f"""Generate a brief, direct, solution-first response about a data science topic.
The response should use technical vocabulary, avoid emojis, and ideally be formatted as bullet points.

Persona characteristics:
- Tone: brief, direct, solution-first
- Style: bullet points, no emojis, technical vocabulary

Example technical concepts to include: dataframe, missing values, machine learning, model evaluation.

Generate a response answering the following question: How do I handle missing values in a pandas DataFrame?
"""

# Step 3: Use the language model to generate text
# Adjust max_new_tokens to control response length
generated_text_raw = generator(prompt, max_new_tokens=100, num_return_sequences=1)[0]['generated_text']

# Step 4: Implement rule-based post-processing
# Simple post-processing: ensure bullet points, remove potential emojis (though prompt should prevent), check for keywords.

# Split into potential lines/points and add bullet points if not already present
processed_lines = []
lines = generated_text_raw.split('\n')
for line in lines:
    cleaned_line = line.strip()
    if cleaned_line and not cleaned_line.startswith('- '):
        processed_lines.append(f"- {cleaned_line}")
    elif cleaned_line:
        processed_lines.append(cleaned_line)

# Join lines back with newlines
generated_text = "\n".join(processed_lines)

# Simple check for keywords (optional, based on criticality)
required_keywords = ["dataframe", "missing values"]
keywords_present = all(keyword in generated_text.lower() for keyword in required_keywords)

print("--- Raw Generated Text ---")
print(generated_text_raw)
print("\n--- Processed Generated Text ---")
print(generated_text)
print(f"\nRequired keywords present: {keywords_present}")

# Step 5: Store the generated text
# The generated_text variable already holds the processed output

"""## Refine and evaluate

### Subtask:
Generate text and compare it to the original examples. Adjust the strategy and implementation as needed to improve the quality of the generated text.

**Reasoning**:
Generate new text using the current implementation and compare it qualitatively against the original examples and the persona definition, then identify areas for improvement.
"""

# Step 1: Generate new text (already done in the previous step, but re-running for clarity)
# Using the existing generator and prompt from the previous step
generated_text_raw = generator(prompt, max_new_tokens=150, num_return_sequences=1)[0]['generated_text'] # Increased tokens slightly

# Step 4: Implement rule-based post-processing (same as before)
processed_lines = []
lines = generated_text_raw.split('\n')
for line in lines:
    cleaned_line = line.strip()
    if cleaned_line and not cleaned_line.startswith('- '):
        processed_lines.append(f"- {cleaned_line}")
    elif cleaned_line:
        processed_lines.append(cleaned_line)

generated_text = "\n".join(processed_lines)

# Step 2 & 3: Compare generated text to original examples and persona definition
print("--- Original Text Examples (Subset) ---")
for i, example in enumerate(text_examples[:3]): # Print a subset for comparison
    print(f"Example {i+1}: {example}")
print("\n--- Generated Text ---")
print(generated_text)

print("\n--- Persona Definition ---")
import json
print(json.dumps(persona_definition, indent=2))

# Qualitative comparison and identification of areas for improvement
# This is a manual step based on the printed output.
# Areas to consider:
# - Tone: Is it brief, direct, solution-first?
# - Style: Does it use bullet points? No emojis? Technical vocabulary?
# - Content: Is it relevant to data science? Does it answer the question?
# - Fluency and Coherence: Does it read naturally?

# Based on observing the output:
# The raw output might contain conversational filler or incomplete sentences.
# The bullet point formatting helps with the 'brief' style.
# Technical vocabulary is present, but could be more specific or varied.
# The answer might be generic depending on the model's output.
# The prompt could be more specific about the desired structure within bullet points.

# Step 4: Identify areas for improvement (manual observation captured above)
# Step 5: Modify strategy/implementation (will be done in the next code block if needed)

"""**Reasoning**:
Based on the qualitative comparison, the generated text includes the prompt itself and repetitive content, indicating the need to refine the prompt and potentially add more robust post-processing to extract only the intended response.


"""

# Step 5: Modify strategy/implementation - Refine the prompt and post-processing

# Refined Prompt: Make it clearer that the model should ONLY provide the answer
refined_prompt = f"""Provide a brief, direct, solution-first answer to the following data science question.
Format the answer using bullet points, use technical vocabulary, and avoid emojis.

Question: How do I handle missing values in a pandas DataFrame?

Answer:
"""

# Use the language model to generate text with the refined prompt
# Adjust max_new_tokens as needed
generated_text_raw_refined = generator(refined_prompt, max_new_tokens=150, num_return_sequences=1)[0]['generated_text']

# Refined Post-processing: Attempt to extract the part after "Answer:" and apply bullet points
# Find the index of "Answer:" in the raw text
answer_start_index = generated_text_raw_refined.find("Answer:")

if answer_start_index != -1:
    # Extract the text after "Answer:"
    answer_text_raw = generated_text_raw_refined[answer_start_index + len("Answer:"):].strip()

    # Apply bullet point formatting to the extracted text
    processed_lines_refined = []
    # Split by newline or sentence-like structures if needed, for now just split by newline
    lines_refined = answer_text_raw.split('\n')
    for line in lines_refined:
        cleaned_line = line.strip()
        if cleaned_line and not cleaned_line.startswith('- '):
            processed_lines_refined.append(f"- {cleaned_line}")
        elif cleaned_line:
            processed_lines_refined.append(cleaned_line)

    generated_text_refined = "\n".join(processed_lines_refined)
else:
    # If "Answer:" is not found, just apply basic bullet point formatting to the whole output
    processed_lines_refined = []
    lines_refined = generated_text_raw_refined.split('\n')
    for line in lines_refined:
        cleaned_line = line.strip()
        if cleaned_line and not cleaned_line.startswith('- '):
            processed_lines_refined.append(f"- {cleaned_line}")
        elif cleaned_line:
            processed_lines_refined.append(cleaned_line)
    generated_text_refined = "\n".join(processed_lines_refined)


# Re-evaluate the generated text
print("--- Generated Text (Refined Strategy) ---")
print(generated_text_refined)

# Qualitative comparison again (manual step)
# - Does it only contain the answer?
# - Is the tone, style, vocabulary better aligned with the persona?

# Based on observing the refined output, check if it's improved.
# If still not satisfactory, further prompt engineering or more advanced post-processing might be needed.

"""## Refine and evaluate - retry 1

### Subtask:
Generate text and compare it to the original examples. Adjust the strategy and implementation as needed to improve the quality of the generated text.

## Summary:

### Data Analysis Key Findings

*   The persona to be cloned was clearly defined, including characteristics like personality traits (helpful, knowledgeable, efficient), interests (data science, programming), tone (brief, direct, solution-first), style (bullet points, no emojis, technical vocabulary), and constraints.
*   A collection of ten text examples relevant to data science and programming was successfully gathered to represent the persona's style.
*   Analysis of the text examples using regex (after overcoming NLTK dependency issues) successfully extracted key features such as common vocabulary, approximate sentence length (average 7.15 words), and common phrases (e.g., "data science", "missing values", "machine learning").
*   A hybrid generation strategy combining a language model with rule-based elements was chosen to generate text mimicking the persona.
*   Initial attempts to implement the text generation using the `distilgpt2` language model and basic post-processing were unsuccessful in producing text that adhered to the defined persona, tone, and style, often including parts of the prompt or generating repetitive content.
*   Further refinement of the prompt and post-processing steps did not resolve the issues, indicating the limitations of the chosen model for this specific persona cloning task.

### Insights or Next Steps

*   The current language model (`distilgpt2`) is not suitable for accurately replicating the defined persona's complex requirements. A more advanced language model, potentially one fine-tuned on technical data or larger in scale, should be used for text generation.
*   More sophisticated rule-based post-processing or guided generation techniques may be necessary to ensure strict adherence to stylistic constraints like bullet points, lack of emojis, and the inclusion of specific technical vocabulary or phrases.
"""

# orchestrator.py
from persona_pipeline import generate_persona_text
from persona_definitions import personas

def orchestrate_persona_response(user_id: str, message: str) -> str:
    persona = personas.get(user_id, personas["default"])
    return generate_persona_text(persona, message)

# persona_definitions.py (or equivalent in notebook)
# Define placeholder personas for now
personas = {
    "default": {
        "tone": "neutral",
        "style": [],
        "tts": None
    }
    # Add other persona definitions here if needed
}

print("Placeholder persona_definitions loaded.")

# persona_pipeline.py (or equivalent in notebook)
# Define a placeholder function for generate_persona_text
def generate_persona_text(persona, message):
    """
    Placeholder function to simulate text generation based on persona.
    Replace with actual persona text generation logic.
    """
    print(f"Generating text for persona with tone: {persona.get('tone', 'default')}")
    return f"This is a placeholder response based on the message: '{message}'"

print("Placeholder generate_persona_text function defined.")

# orchestrator.py - Original code with imports that should now work
from persona_pipeline import generate_persona_text
from persona_definitions import personas

def orchestrate_persona_response(user_id: str, message: str) -> str:
    persona = personas.get(user_id, personas["default"])
    return generate_persona_text(persona, message)

print("\nOrchestrator function defined. You can now call orchestrate_persona_response.")
# Example usage (optional, can be in a separate cell)
# test_user_id = "default"
# test_message = "Tell me about data science."
# response = orchestrate_persona_response(test_user_id, test_message)
# print(f"\nOrchestrated Response: {response}")

from google.colab import userdata
import os

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# persona_pipeline.py
from pydantic import BaseModel
from openai import OpenAI
import re
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class PersonaSchema(BaseModel):
    name: str
    style: dict  # {"bullet_points": True, "no_emojis": True, "required_vocab": ["FastAPI", "LLM"]}
    tone: str
    knowledge_domains: list[str]

def post_process(text: str, style: dict) -> str:
    if style.get("bullet_points"):
        text = re.sub(r"(?m)^(?!- )", "- ", text)  # enforce bullet start
    if style.get("no_emojis"):
        text = re.sub(r"[^\x00-\x7F]+", "", text)  # strip emojis & non-ASCII
    if "required_vocab" in style:
        for word in style["required_vocab"]:
            if word.lower() not in text.lower():
                text += f"\n- {word}"  # append if missing
    return text.strip()

def generate_persona_text(persona: PersonaSchema, user_message: str) -> str:
    prompt = f"""
    You are {persona.name}.
    Tone: {persona.tone}.
    Knowledge domains: {', '.join(persona.knowledge_domains)}.
    Respond strictly in line with style rules: {persona.style}.
    User: {user_message}
    """

    response = client.chat.completions.create(
        model="gpt-4o",  # production-grade upgrade
        messages=[
            {"role": "system", "content": "You are a highly specialized persona generator."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500
    )

    raw_text = response.choices[0].message.content
    return post_process(raw_text, persona.style)

# main.py (FastAPI entry point)
from fastapi import FastAPI, Request
# orchestrator import removed

app = FastAPI()

@app.post("/webhook/web")
async def web_webhook(req: Request):
    data = await req.json()
    user_id = data.get("user_id", "default")
    text = data.get("text", "")
    # Directly call the function defined in a previous cell
    reply = orchestrate_persona_response(user_id, text)
    return {"reply": reply}

!pip install openai pydantic fastapi uvicorn
!uvicorn main:app --reload